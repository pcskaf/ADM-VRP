{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-be4088e3d50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgmtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mattention_dynamic_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttentionDynamicModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_decode_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "from attention_dynamic_model import AttentionDynamicModel, set_decode_type\n",
    "from reinforce_baseline import RolloutBaseline\n",
    "from train import train_model\n",
    "\n",
    "from utils import create_data_on_disk, get_cur_time\n",
    "\n",
    "# Params of model\n",
    "SAMPLES = 512 # 128*10000\n",
    "BATCH = 128\n",
    "START_EPOCH = 0\n",
    "END_EPOCH = 100\n",
    "FROM_CHECKPOINT = False\n",
    "embedding_dim = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "ROLLOUT_SAMPLES = 10000\n",
    "NUMBER_OF_WP_EPOCHS = 1\n",
    "GRAD_NORM_CLIPPING = 1.0\n",
    "BATCH_VERBOSE = 1000\n",
    "VAL_BATCH_SIZE = 10000\n",
    "VALIDATE_SET_SIZE = 10000\n",
    "SEED = 1234\n",
    "GRAPH_SIZE = 20\n",
    "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
    "\n",
    "# Initialize model\n",
    "model_tf = AttentionDynamicModel(embedding_dim)\n",
    "set_decode_type(model_tf, \"sampling\")\n",
    "print(get_cur_time(), 'model initialized')\n",
    "\n",
    "# Create and save validation dataset\n",
    "validation_dataset = create_data_on_disk(GRAPH_SIZE,\n",
    "                                         VALIDATE_SET_SIZE,\n",
    "                                         is_save=True,\n",
    "                                         filename=FILENAME,\n",
    "                                         is_return=True,\n",
    "                                         seed = SEED)\n",
    "print(get_cur_time(), 'validation dataset created and saved on the disk')\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# Initialize baseline\n",
    "baseline = RolloutBaseline(model_tf,\n",
    "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
    "                           epoch = 0,\n",
    "                           num_samples=ROLLOUT_SAMPLES,\n",
    "                           filename = FILENAME,\n",
    "                           from_checkpoint = FROM_CHECKPOINT,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           graph_size=GRAPH_SIZE\n",
    "                           )\n",
    "print(get_cur_time(), 'baseline initialized')\n",
    "\n",
    "start=time.time()\n",
    "train_model(optimizer,\n",
    "            model_tf,\n",
    "            baseline,\n",
    "            validation_dataset,\n",
    "            samples = SAMPLES,\n",
    "            batch = BATCH,\n",
    "            val_batch_size = VAL_BATCH_SIZE,\n",
    "            start_epoch = START_EPOCH,\n",
    "            end_epoch = END_EPOCH,\n",
    "            from_checkpoint = FROM_CHECKPOINT,\n",
    "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
    "            batch_verbose = BATCH_VERBOSE,\n",
    "            graph_size = GRAPH_SIZE,\n",
    "            filename = FILENAME\n",
    "            )\n",
    "print(\"Total Training Time: \", time.time()-start, \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a7a92707ad52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgmtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mattention_dynamic_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_decode_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreinforce_baseline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRolloutBaseline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from attention_dynamic_model import set_decode_type\n",
    "from reinforce_baseline import RolloutBaseline\n",
    "from train import train_model\n",
    "\n",
    "from utils import get_cur_time\n",
    "from reinforce_baseline import load_tf_model\n",
    "from utils import read_from_pickle\n",
    "\n",
    "\n",
    "SAMPLES = 512 # 128*10000\n",
    "BATCH = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "ROLLOUT_SAMPLES = 10000\n",
    "NUMBER_OF_WP_EPOCHS = 1\n",
    "GRAD_NORM_CLIPPING = 1.0\n",
    "BATCH_VERBOSE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "VALIDATE_SET_SIZE = 10000\n",
    "SEED = 1234\n",
    "GRAPH_SIZE = 20\n",
    "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
    "\n",
    "START_EPOCH = 5\n",
    "END_EPOCH = 10\n",
    "FROM_CHECKPOINT = True\n",
    "embedding_dim = 128\n",
    "MODEL_PATH = 'model_checkpoint_epoch_4_VRP_50_2020-06-08.h5'\n",
    "VAL_SET_PATH = 'Validation_dataset_VRP_50_2020-06-08.pkl'\n",
    "BASELINE_MODEL_PATH = 'baseline_checkpoint_epoch_0_VRP_50_2020-06-08.h5'\n",
    "\n",
    "# Initialize model\n",
    "model_tf = load_tf_model(MODEL_PATH,\n",
    "                         embedding_dim=embedding_dim,\n",
    "                         graph_size=GRAPH_SIZE)\n",
    "set_decode_type(model_tf, \"sampling\")\n",
    "print(get_cur_time(), 'model loaded')\n",
    "\n",
    "# Create and save validation dataset\n",
    "validation_dataset = read_from_pickle(VAL_SET_PATH)\n",
    "print(get_cur_time(), 'validation dataset loaded')\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# Initialize baseline\n",
    "baseline = RolloutBaseline(model_tf,\n",
    "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
    "                           epoch = START_EPOCH,\n",
    "                           num_samples=ROLLOUT_SAMPLES,\n",
    "                           filename = FILENAME,\n",
    "                           from_checkpoint = FROM_CHECKPOINT,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           graph_size=GRAPH_SIZE,\n",
    "                           path_to_checkpoint = BASELINE_MODEL_PATH)\n",
    "print(get_cur_time(), 'baseline initialized')\n",
    "\n",
    "train_model(optimizer,\n",
    "            model_tf,\n",
    "            baseline,\n",
    "            validation_dataset,\n",
    "            samples = SAMPLES,\n",
    "            batch = BATCH,\n",
    "            val_batch_size = VAL_BATCH_SIZE,\n",
    "            start_epoch = START_EPOCH,\n",
    "            end_epoch = END_EPOCH,\n",
    "            from_checkpoint = FROM_CHECKPOINT,\n",
    "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
    "            batch_verbose = BATCH_VERBOSE,\n",
    "            graph_size = GRAPH_SIZE,\n",
    "            filename = FILENAME\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
